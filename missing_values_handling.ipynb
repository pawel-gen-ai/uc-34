{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62da3e36-2656-47e0-9612-efdd25fae787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[346, 109, 56, 865, 457]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark \n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "# do something to prove it works\n",
    "rdd = sc.parallelize(range(1000))\n",
    "rdd.takeSample(False, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3250d23b-a7f5-4c05-8af5-1c3b8170a08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#df: 499999\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV File\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"work/clean_me_out.csv\", header=True, sep=\",\", quote = \"\\\"\")\n",
    "print(f\"#df: {df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9be5e28f-45db-412a-b49f-b147e911c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into table\n",
    "df.createOrReplaceTempView(\"clean_me\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f8f8dad-c228-4b81-9b30-530a8e0398f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare average data\n",
    "avg_a =spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            delivery_company\n",
    "            ,avg(CAST(quantity AS int)) avg_quantity\n",
    "        FROM clean_me\n",
    "        GROUP BY delivery_company\n",
    "    \"\"\").collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb260529-aaae-4613-bfdd-bd220b309821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep dictionary with averages\n",
    "avg_dict = {r[\"delivery_company\"]:r[\"avg_quantity\"] for r in avg_a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b10cbff-003c-41ce-944c-b8cdf19f5f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'delivery_comp_1': 1.5,\n",
       " 'delivery_comp_3': 1.499993999976,\n",
       " 'delivery_comp_2': 1.500006000024,\n",
       " 'delivery_comp_0': 1.499993999976}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be8e097f-2f1b-4a69-a4d8-694da5522be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function replace null quantity\n",
    "def replace_quantity(company, quantity, av_d):\n",
    "    if company in av_d:\n",
    "        #null is considered as None, \"null\",  \"#NA\"\n",
    "        if (quantity is None) or (quantity == \"null\") or (quantity == \"#NA\"):\n",
    "            return av_d[company]\n",
    "    return quantity\n",
    "\n",
    "#print( replace_quantity('delivery_comp_1a', 5 , avg_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "161c1c38-42cf-4963-b691-47035d002b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "927e30e3-8e5e-49c9-b92d-d2e6a1fb2ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_quantityUDF = udf(lambda z, y : replace_quantity(z, y, avg_dict)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cfdef92-33d7-4b06-bb04-5921c888d83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+------------------+\n",
      "|delivery_company|quantity|quantity_converted|\n",
      "+----------------+--------+------------------+\n",
      "| delivery_comp_1|       1|                 1|\n",
      "| delivery_comp_2|       2|                 2|\n",
      "| delivery_comp_3|    null|    1.499993999976|\n",
      "| delivery_comp_0|       1|                 1|\n",
      "| delivery_comp_1|       2|                 2|\n",
      "| delivery_comp_2|     #NA|    1.500006000024|\n",
      "| delivery_comp_3|       1|                 1|\n",
      "| delivery_comp_0|       2|                 2|\n",
      "| delivery_comp_1|     #NA|               1.5|\n",
      "| delivery_comp_2|       1|                 1|\n",
      "+----------------+--------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test to check what is in dataset\n",
    "df.select(col(\"delivery_company\"), col(\"quantity\"), replace_quantityUDF(col(\"delivery_company\"), col(\"quantity\")).alias(\"quantity_converted\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5419f5e7-c6d4-4d37-8c57-d786cfab59bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- delivery_company: string (nullable = true)\n",
      " |-- ordered_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, last_day, coalesce\n",
    "#fix dates in dataset\n",
    "def unify_date(date):\n",
    "    if date == 'null':\n",
    "        return None\n",
    "    #replace slahs with dash\n",
    "    words = date.replace(\"/\",\"-\").split(\"-\")\n",
    "    #add leading zeroes\n",
    "    return f\"{('0' + words[0])[-2:]}-{('0'+ words[1])[-2:]}-{words[2]}\"\n",
    "    \n",
    "udf_unify_date = udf(unify_date)\n",
    "\n",
    "#prepare dataset with corrected dates\n",
    "df2 = df.select(\"order_id\", \"delivery_company\", to_date(udf_unify_date(\"ordered_date\"), \"dd-MM-yyyy\").alias(\"ordered_date\") )\n",
    "df2.createOrReplaceTempView(\"clean_me2\")\n",
    "df2.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c82889d5-774a-4d79-ad1a-91eb344eee7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-03-04'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prep data for orders date\n",
    "from datetime import datetime\n",
    "df_cor = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            order_id\n",
    "            ,delivery_company\n",
    "            ,ordered_date\n",
    "            ,CASE WHEN ordered_date is NULL \n",
    "                THEN  COALESCE(\n",
    "                            LAG(ordered_date) IGNORE NULLS OVER (partition by delivery_company order by order_id) + 1\n",
    "                            ,last_day(LAG(ordered_date) IGNORE NULLS OVER (partition by delivery_company order by order_id DESC))\n",
    "                        )\n",
    "                ELSE  ordered_date\n",
    "                END as corrected_ordered_date\n",
    "        FROM clean_me2\n",
    "    \"\"\")\n",
    "df_cor = df_cor.filter(\"ordered_date is NULL\").select(\"order_id\", \"corrected_ordered_date\")\n",
    "cor_date = df_cor.collect()\n",
    "#dictionary with proper dates values for proper order id \n",
    "cor_dict = {r[\"order_id\"]:r[\"corrected_ordered_date\"].strftime(\"%Y-%m-%d\") for r in cor_date}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "29cbc79e-cfa9-45cd-878a-54d079891e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-04\n"
     ]
    }
   ],
   "source": [
    "#function with correct dates for null dates\n",
    "def correct_dates(id, old_value):\n",
    "    if id in cor_dict:\n",
    "        return cor_dict[id] \n",
    "    return old_value\n",
    "    \n",
    "\n",
    "correct_datesUDF = udf(lambda z, y : correct_dates(z, y)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "47b26831-e109-456a-a925-e4f07d4a6ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a1e15c71-ab77-449c-a2b7-150043729954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+------------+----------------------+\n",
      "|order_id|delivery_company|ordered_date|corrected_ordered_date|\n",
      "+--------+----------------+------------+----------------------+\n",
      "|1       |delivery_comp_1 |2022-02-09  |2022-02-09            |\n",
      "|2       |delivery_comp_2 |null        |2022-03-04            |\n",
      "|3       |delivery_comp_3 |2022-03-14  |2022-03-14            |\n",
      "|4       |delivery_comp_0 |2022-04-20  |2022-04-20            |\n",
      "|5       |delivery_comp_1 |null        |2022-04-10            |\n",
      "|6       |delivery_comp_2 |null        |2022-04-22            |\n",
      "|7       |delivery_comp_3 |2022-02-20  |2022-02-20            |\n",
      "|8       |delivery_comp_0 |2022-04-01  |2022-04-01            |\n",
      "|9       |delivery_comp_1 |2022-04-13  |2022-04-13            |\n",
      "|10      |delivery_comp_2 |null        |2022-02-28            |\n",
      "+--------+----------------+------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "df2.select(col(\"order_id\"), col(\"delivery_company\"), col(\"ordered_date\"), correct_datesUDF(col(\"order_id\"), date_format(col(\"ordered_date\"), \"yyyy-MM-dd\")).alias(\"corrected_ordered_date\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5549ea6-73bf-4712-9e0c-21c0273cd40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
